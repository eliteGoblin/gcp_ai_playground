# AI Coach Evaluation Framework

> Design document for measuring and progressively improving the AI Coach system.
> This is a reference for post-MVP evaluation - implementation follows E2E working system.

---

## The Core Challenge: Evaluating an Evaluator

Unlike traditional AI systems with clear ground truth, an AI Coach evaluates subjective human performance. There's no "correct answer" for coaching feedback.

```
Traditional AI:                           AI Coach:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Query   â”‚ â”€â”€â–¶ â”‚ Answer â”‚              â”‚Transcriptâ”‚ â”€â”€â–¶ â”‚ Assessment â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚ + Coaching â”‚
                    â”‚                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–¼                                          â”‚
              Ground Truth                                     â–¼
              (correct answer)                            ??? ğŸ¤”
                                                   What's "ground truth"?
```

**Evaluation sources (all imperfect):**
- Expert QA review - expensive, subjective
- Agent improvement over time - lagging indicator, noisy
- Business outcomes - confounded by many factors

---

## Success Definition: Business Terms First

From the book summary - define success in business impact, not "cool demo":

| Business Metric | Definition | How AI Coach Helps |
|----------------|------------|-------------------|
| **QA Coverage** | % of calls reviewed | 2% â†’ 100% (AI reviews all) |
| **Feedback Latency** | Time from call to feedback | 3-5 days â†’ <1 hour |
| **Cost per Review** | Human time + tools | ~$5-10 â†’ ~$0.10 |
| **Compliance Breach Rate** | Violations per 1000 calls | Track reduction |
| **Agent Improvement** | Score trends over time | Track positive trend |

### The Coverage vs Accuracy Tradeoff

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                      â”‚
â”‚  KEY INSIGHT:                                                        â”‚
â”‚                                                                      â”‚
â”‚  70% accuracy AI at 100% coverage may beat                          â”‚
â”‚  95% accuracy human at 2% coverage                                  â”‚
â”‚                                                                      â”‚
â”‚  Why? Most calls go unreviewed today.                               â”‚
â”‚  A "good enough" AI catches more issues than perfect human sampling.â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 4-Layer Metrics Framework

### Layer 1: System Quality (Can We Trust It?)

*"Is the coach's assessment correct?"*

| Metric | What It Measures | Collection Method | Target |
|--------|------------------|-------------------|--------|
| **Human Agreement Rate** | % coach assessments experts agree with | Sample 10-20/week, QA scores same rubric blind | >80% |
| **Consistency Score** | Same input â†’ same output | Run same conversation 3x | <5% variance |
| **Citation Accuracy** | Policy citations are correct | Verify policy_ref against docs | 100% |
| **False Positive Rate** | Flagged violations that aren't | QA reviews flagged convos | <10% |
| **False Negative Rate** | Missed real violations | Audit "clean" conversations | <5% |

**Human Agreement Workflow:**
```
Weekly Calibration:
1. Sample 10-20 conversations (stratified by score buckets)
2. 2+ QA experts score using same rubric (blind to AI output)
3. Compare AI scores vs Expert scores
4. Calculate agreement rate, identify patterns
5. Update prompts/rubric based on systematic disagreements

Agreement Types:
â”œâ”€â”€ Exact: AI score == Expert score
â”œâ”€â”€ Within-1: abs(AI - Expert) <= 1 (for 1-10 scales)
â””â”€â”€ Directional: Both agree on good/needs-improvement/critical
```

### Layer 2: Coaching Usefulness (Is It Helpful?)

*"Will agents find this valuable?"*

| Metric | What It Measures | Collection Method | Target |
|--------|------------------|-------------------|--------|
| **Coaching Specificity** | Specific vs generic advice | LLM/human judges "is this actionable?" | >90% |
| **Evidence Quality** | Cites specific transcript moments | Check turn references in output | 100% |
| **Novelty Rate** | Finds issues QA missed | Compare vs human review | Track |
| **Agent Feedback Score** | Agent rates helpfulness | Survey after viewing report | >4.0/5 |
| **Adoption Rate** | % of points agents will apply | Track in interface | >60% |

**Specificity Examples:**

```
BAD (Generic):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ "Show more empathy to customers experiencing hardship"          â”‚
â”‚                                                                  â”‚
â”‚ âŒ No specific moment cited                                      â”‚
â”‚ âŒ No suggested alternative phrasing                             â”‚
â”‚ âŒ Could apply to any call                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

GOOD (Specific):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ "At turn 13, when Mr Chen mentioned his wife's dialysis, you    â”‚
â”‚  immediately pivoted to payment options.                         â”‚
â”‚                                                                  â”‚
â”‚  Try: 'I'm sorry to hear about your wife's situation. That      â”‚
â”‚  must be incredibly stressful. Before we discuss payment, is    â”‚
â”‚  there anything urgent I can help with today?'"                  â”‚
â”‚                                                                  â”‚
â”‚ âœ… Cites specific turn                                           â”‚
â”‚ âœ… Identifies exact behavior                                     â”‚
â”‚ âœ… Provides alternative phrasing                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Layer 3: Business Impact (Does It Work?)

*"Are agents actually improving?"*

| Metric | What It Measures | Collection Method | Target |
|--------|------------------|-------------------|--------|
| **Agent Score Trend** | Empathy/compliance over weeks | Track by agent_id over time | Positive |
| **Repeat Issues Rate** | Same issue flagged repeatedly | Track issue_types by agent | Decreasing |
| **Compliance Breach Rate** | Org-wide violation % | Aggregate from coach_analysis | Decreasing |
| **Customer Sentiment** | CI sentiment by agent | Aggregate from ci_enrichment | Improving |
| **Escalation Rate** | Calls with escalation triggers | Track ci_flags by agent | Decreasing |

**Agent Improvement Visualization:**
```
Agent M7741 - Empathy Score Over Time:

Score
 10 â”‚
  9 â”‚                                              â—
  8 â”‚                                    â—    â—
  7 â”‚                         â—    â—
  6 â”‚              â—    â—
  5 â”‚    â—    â—                                        â† Coaching started
  4 â”‚
    â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â–¶ Week
        W1   W2   W3   W4   W5   W6   W7   W8
```

### Layer 4: Operational Health (System Status)

*"Is the system running well?"*

| Metric | What It Measures | Collection Method | Target |
|--------|------------------|-------------------|--------|
| **Latency P50/P95** | Time to generate coaching | Pipeline timestamps | P95 <30s |
| **Cost per Conversation** | LLM + infra cost | Token usage + compute | <$0.10 |
| **Throughput** | Conversations/hour | Pipeline metrics | >100/hr |
| **Error Rate** | Failed generations | Pipeline error logs | <1% |
| **Staleness** | Call end â†’ coaching available | Timestamp comparison | <24hr |

---

## Progressive Evaluation Roadmap

### Phase 1: MVP Demo (Current)
Focus: *Get it working E2E*

```
Evaluation: Manual spot-checks only
â”œâ”€â”€ Review 5-10 coaching outputs manually
â”œâ”€â”€ Check coaching makes sense for transcript
â”œâ”€â”€ Verify policy citations exist
â””â”€â”€ No formal metrics yet
```

### Phase 2: Pilot (Post-Demo)
Focus: *Establish baselines*

```
Evaluation: Weekly calibration
â”œâ”€â”€ Sample 20 conversations/week for QA review
â”œâ”€â”€ Calculate initial agreement rate
â”œâ”€â”€ Track latency and error rates
â”œâ”€â”€ Collect qualitative feedback
â””â”€â”€ Iterate on prompts based on disagreements
```

### Phase 3: Production (Scale)
Focus: *Continuous improvement flywheel*

```
Evaluation: Full framework
â”œâ”€â”€ Automatic evals on 100% (Tier 1)
â”œâ”€â”€ Sampled human review 5-10% (Tier 2)
â”œâ”€â”€ Deep dive on anomalies (Tier 3)
â”œâ”€â”€ Weekly metrics dashboard
â””â”€â”€ Monthly calibration reviews
```

---

## Tiered Evaluation Approach

The evaluation paradox: We build AI to replace expensive human QA, but need expensive human QA to evaluate the AI.

**Solution: Focus human effort where it matters most**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TIER 1: Automatic (100% of conversations)                          â”‚
â”‚ â”œâ”€â”€ Consistency checks (same input â†’ same output?)                 â”‚
â”‚ â”œâ”€â”€ Format validation (required fields populated?)                 â”‚
â”‚ â”œâ”€â”€ Citation verification (policy_ref exists?)                     â”‚
â”‚ â”œâ”€â”€ LLM-as-judge for specificity                                   â”‚
â”‚ â””â”€â”€ CI signal correlation (phrase_match â†’ compliance_issue?)       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ TIER 2: Sampled Human Review (5-10% of conversations)              â”‚
â”‚ â”œâ”€â”€ Random sample from each score bucket                           â”‚
â”‚ â”œâ”€â”€ Oversample edge cases (score=1 or score=10)                    â”‚
â”‚ â”œâ”€â”€ Oversample flagged conversations                               â”‚
â”‚ â””â”€â”€ Weekly calibration sessions                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ TIER 3: Deep Dive (Triggered by anomalies)                         â”‚
â”‚ â”œâ”€â”€ Agent disputes coaching                                         â”‚
â”‚ â”œâ”€â”€ Score dramatically different from CI signals                   â”‚
â”‚ â”œâ”€â”€ Repeat violations after coaching                                â”‚
â”‚ â””â”€â”€ System errors or timeouts                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Continuous Improvement Flywheel

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ 1. Generate     â”‚
                    â”‚    Coaching     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                    â”‚                    â”‚
        â–¼                    â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2a. QA Sample â”‚  â”‚ 2b. Agent     â”‚  â”‚ 2c. Automatic â”‚
â”‚     Review    â”‚  â”‚    Feedback   â”‚  â”‚     Evals     â”‚
â”‚               â”‚  â”‚               â”‚  â”‚               â”‚
â”‚ 10-20/week    â”‚  â”‚ Survey after  â”‚  â”‚ LLM-as-judge  â”‚
â”‚ Expert score  â”‚  â”‚ viewing       â”‚  â”‚ Consistency   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                  â”‚                  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ 3. Analyze Disagreementsâ”‚
              â”‚    & Failure Modes      â”‚
              â”‚                         â”‚
              â”‚ â€¢ Where AI != human     â”‚
              â”‚ â€¢ What issues missed?   â”‚
              â”‚ â€¢ Generic vs specific?  â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ 4. Update System        â”‚
              â”‚                         â”‚
              â”‚ â€¢ Refine prompts        â”‚
              â”‚ â€¢ Add examples to RAG   â”‚
              â”‚ â€¢ Adjust rubric         â”‚
              â”‚ â€¢ Add phrase matchers   â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Back to Step 1
```

---

## Launch Readiness Checklist

### MVP/Demo (Minimum Bar)

- [ ] Human agreement rate >70% (on 20 conversation sample)
- [ ] All compliance flags have evidence citations
- [ ] Coaching points are specific (not generic platitudes)
- [ ] Latency <60s for 95% of conversations
- [ ] No hallucinated policy citations (100% accuracy)

### Production (Nice to Have)

- [ ] Human agreement rate >85%
- [ ] Agent feedback score >4.0/5.0
- [ ] Measurable agent improvement trend over 4 weeks
- [ ] Cost <$0.10 per conversation
- [ ] Latency <30s P95

---

## Future: Evaluation Data Model

When ready to implement formal evaluation tracking:

### coach_calibration table
```
calibration_id      STRING (REQUIRED)
conversation_id     STRING
calibration_date    DATE

# AI scores
ai_empathy_score         INTEGER
ai_compliance_score      INTEGER
ai_overall_score         INTEGER
ai_issue_count           INTEGER

# Expert scores
expert_empathy_score     INTEGER
expert_compliance_score  INTEGER
expert_overall_score     INTEGER
expert_issue_count       INTEGER
expert_id                STRING

# Agreement metrics
empathy_agreement        STRING  -- exact/within1/disagree
compliance_agreement     STRING
issues_agreement         FLOAT   -- Jaccard similarity

# Notes
disagreement_notes       STRING
action_taken             STRING  -- prompt_updated, rubric_changed
```

### coach_feedback table
```
feedback_id         STRING (REQUIRED)
conversation_id     STRING
agent_id            STRING
feedback_date       TIMESTAMP

helpfulness_score   INTEGER  -- 1-5
specificity_score   INTEGER  -- 1-5
will_apply          BOOLEAN

comments            STRING
coaching_point_id   STRING  -- which point feedback is about
```

---

## Summary: Mental Model

1. **Start with business value** - coverage Ã— accuracy, not just accuracy
2. **Layer your metrics** - system quality â†’ usefulness â†’ business impact â†’ operations
3. **Tier your evaluation** - auto checks for all, human review for sample, deep dive for anomalies
4. **Build the flywheel** - generate â†’ evaluate â†’ analyze â†’ improve â†’ repeat
5. **Progressive rollout** - spot checks (MVP) â†’ calibration (pilot) â†’ full framework (prod)

**Key insight**: Don't block on perfect evaluation. Get E2E working, then progressively add rigor.

---

## References

- Book: "Quick read summary (highlights)" on AI system evaluation
- Related: `design/solution_roadmap.md` - implementation plan
- Related: `cc_coach/schemas/coach_analysis.json` - output schema
